---
title: "R Notebook"
output: pdf_output
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(twitteR)
library(ROAuth)
library(tidyverse)
library(purrrlyr)
library(text2vec)
library(caret)
library(glmnet)
library(ggrepel)
```

```{r loading data}
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
tweets_classified <- read_csv('training.1600000.processed.noemoticon.csv', col_names = c('sentiment', 'id', 'date', 'query', 'user', 'text')) %>%
  # converting some symbols
  dmap_at('text', conv_fun) %>%
  # replacing class values
  mutate(sentiment = ifelse(sentiment == 0, 0, 1))

head(tweets_classified,15)
```

```{r splitting data}
set.seed(2340)
trainIndex <- createDataPartition(tweets_classified$sentiment, p = 0.8, 
                                  list = FALSE, 
                                  times = 1)
tweets_train <- tweets_classified[trainIndex, ]
tweets_test <- tweets_classified[-trainIndex, ]
```

```{r}
tweets_train$sentiment
```

```{r vectorization}
prep_fun <- tolower
tok_fun <- word_tokenizer

it_train <- itoken(tweets_train$text, 
                   preprocessor = prep_fun, 
                   tokenizer = tok_fun,
                   ids = tweets_train$id,
                   progressbar = TRUE)
it_test <- itoken(tweets_test$text, 
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun,
                  ids = tweets_test$id,
                  progressbar = TRUE)
```

```{r creating vocab}
vocab <- create_vocabulary(it_train)
```

```{r }
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
```

```{r }
tfidf <- TfIdf$new()
```


```{r }
# fit the model to the train data and transform it with the fitted model
dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf  <- create_dtm(it_test, vectorizer) %>% 
  transform(tfidf)
```


```{r word2vec}
t1 <- Sys.time()
glmnet_classifier <- cv.glmnet(x = dtm_train_tfidf,
                               y = tweets_train[['sentiment']], 
                               family = 'binomial', 
                               # L1 penalty
                               alpha = 1,
                               # interested in the area under ROC curve
                               type.measure = "auc",
                               # 5-fold cross-validation
                               nfolds = 5,
                               # high value is less accurate, but has faster training
                               thresh = 1e-3,
                               # again lower number of iterations for faster training
                               maxit = 1e3)
```

```{r}
print(difftime(Sys.time(), t1, units = 'mins'))
```
```{r}
plot(glmnet_classifier)
print(paste("max AUC(Area under the curve) =", round(max(glmnet_classifier$cvm),4)))
```


```{r}
preds
```

```{r }
preds <- predict(glmnet_classifier, dtm_test_tfidf, type = 'response')[ ,1]
as.factor(tweets_test$sentiment)
auc(as.numeric(tweets_test$sentiment), preds)
```

```{r }
rm(list = setdiff(ls(), c('glmnet_classifier', 'conv_fun', 'prep_fun', 'tok_fun', 'vectorizer', 'tfidf')))
save.image('image.RData')
rm(list = ls())
```

```{r }
load('image.RData')

```



```{r}
api_key = "eNZ7MHsj6VdR9eHteoAlc8NOl"
api_secret = "2mOEHHqoSEKMYtZIYVAaEzLAJVPeyoYIZXe5tc0W38mfCogi9G"
access_token = "1183418925188468736-tIv2hmgTGwRpOHubo8323LsEpwPtq0"
access_token_secret = "cCgIcxMx6LAcTJH067GUUKAkTg39eHlJ36KdNFYgVWz7P"

setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)

```

```{r }
df_tweets <- twListToDF(searchTwitter('G7 OR #G7', n = 1000, lang = 'en')) %>% dmap_at('text', conv_fun)

# preprocessing and tokenization
it_tweets <- itoken(df_tweets$text,
                    preprocessor = prep_fun,
                    tokenizer = tok_fun,
                    ids = df_tweets$id,
                    progressbar = TRUE)

# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)
```


```{r }
# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)

# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]

# adding rates to initial dataset
df_tweets$sentiment <- preds_tweets
```


```{r}
text = "You"
nit_tweets <- itoken(text,
                    preprocessor = prep_fun,
                    tokenizer = tok_fun,
                    progressbar = TRUE)
ndtm_tweets <- create_dtm(nit_tweets, vectorizer)
ndtm_tweets_tfidf <- fit_transform(ndtm_tweets, tfidf)

# predict probabilities of positiveness
npreds_tweets <- predict(glmnet_classifier, ndtm_tweets_tfidf, type = 'response')[ ,1]
npreds_tweets


```