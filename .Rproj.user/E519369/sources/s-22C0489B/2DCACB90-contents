---
title: "Group 11 - Assignment 5"
author: "Abhijit Krishna Menon and Manya Raman"
date: "November 17, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width=30)
```

```{r wrap-hook, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```


```{r installing packages, message=FALSE}
library(xlsx)
library("openxlsx") 
library(rsample) 
library(dplyr) 
library(rpart) 
library(rpart.plot) 
library(ipred) 
library(caret)
library(dummies)

```

# Problem 1 (Predicting Price of Used Car, CART) [35 points]

The file ToyotaCorolla.xlsx contains the data on used cars (Toyota Corolla) on sale during late summer of 2004 in The Netherlands. It has 1436 records containing details on 38 attributes, including Price, Age, Kilometers, HP, and other specifications. The goal is to predict the price of a used Toyota Corolla based on its specifications.

```{r read_toyota}
Toyota_data<-read.xlsx("ToyotaCorolla.xlsx",sheet = "data")
head(Toyota_data[,-1])
```


```{r clearing_null}
Toyota_data_new = Toyota_data[,-1]
Toyota_data_new = na.omit(Toyota_data_new)
head(str(Toyota_data_new))
```

Data Preprocessing: Create dummy variables for the categorical predictors (Fuel Type and Color). Split the data into training (50%), validation (30%), and test (20%) datasets.

```{r factorising data}
Toyota_data_new$Fuel_Type <- factor(Toyota_data_new$Fuel_Type)
Toyota_data_new$Color <- factor(Toyota_data_new$Color)
```

```{r dummy_vars}
Toyota_data_dummy <- dummy.data.frame(Toyota_data_new,
                                      names = c("Fuel_Type","Color") , sep = ".")
head(Toyota_data_dummy)
```


```{r sampling_toyota }
set.seed(999)
Split_dataset = sample(1:3, size = nrow(Toyota_data_dummy),
                       prob = c(0.5,0.3,0.2), replace=TRUE)
training_data = Toyota_data_dummy[Split_dataset == 1,]
validation_data = Toyota_data_dummy[Split_dataset == 2,]
test_data = Toyota_data_dummy[Split_dataset == 3,]
```

a. Run a regression tree (RT) with the output variable Price and input variables
Age_08_04, KM, Fuel_Type, HP, Automatic, Doors, Quarterly_Tax, Mfg_Guarantee, Guarantee_Period, Airco, Automatic_Airco, CD Player, Powered_Windows, Sport_Model, and Tow_Bar.

```{r viz_toy_regression_tree}
regression_tree <- rpart(Price ~ Age_08_04 + KM + Fuel_Type.CNG + Fuel_Type.Diesel + 
                           Fuel_Type.Petrol + HP + Automatic + Doors + 
                           Quarterly_Tax + Mfr_Guarantee + Guarantee_Period +
                           Airco + Automatic_airco + CD_Player + 
                           Powered_Windows + Sport_Model + 
                           Tow_Bar, data = training_data,  method = "anova")
summary(regression_tree)
                         
```

```{r cross_val_plot}
plotcp(regression_tree)
```

```{r regression_tree}
prp(regression_tree, type = 1, extra = 1, under = TRUE, branch = 0, split.font = 1,cex =1 ,varlen = -10)
```

## i. Which appear to be the three or four most important car specifications for predicting the car’s price?

The regression tree shows that the four most important variables are: Age_08_04, HP , Automatic and KM.


## ii. Compare the prediction errors of the training, validation, and test sets by examining their RMS error and by plotting the three boxplots. What is happening with the training set predictions? How does the predictive performance of the test set compare to the other two? Why does this occur?


```{r rmse_calc}
parted_data<- list(training_data,validation_data,test_data)
graph_names <- c("Train","Validation","Test")
RMSE <- vector()
for (j in 1:3){
pred <- predict(regression_tree,parted_data[[j]])
Error <- pred - parted_data[[j]][2]
SqErr <- Error^2
boxplot(Error, xlab = graph_names[j], ylab = "Error", 
        ylim = c(-11000,6000))
RMSE <- c(RMSE, sqrt(mean(SqErr[,1])))
}
RMSE
```
The distribution in the boxplots for all three datasets are very similar; however, validation and test dataset contains more outliers, where validation dataset has three of the farthest of all. As expected, training dataset because the regression tree used it.

The training data has the lowest RMSE, validation comes in second and the test data comes in 3rd. 


##  iii. If we used the full tree instead of the best pruned tree to score the validation set, how would this affect the predictive performance for the validation set? (Hint: Does the full tree use the validation data?)

The best pruned tree will avoid overfitting the model to the training data. Therefore the predictive performance on a completely new dataset(the validation data) will be much higher for a pruned tree as compared to an over fitted model by a full tree. 


-----------------------------------------------------------------------------


## b. Let us see the effect of turning the price variable into a categorical variable. First, create a new variable that categorizes price into 20 bins of equal counts. Now repartition the data keeping Binned Price instead of Price. Run a classification tree (CT) with the same set of input variables as in the RT, and with Binned Price as the output variable.

```{r binning_price}
Toyota_data_dummy$bin_price = as.factor(as.numeric(cut(Toyota_data_dummy$Price,20)))
Toyota_data_class <- Toyota_data_dummy[,-2]
```

```{r classification_split}
training_data_c <- Toyota_data_class[Split_dataset==1,] 
validation_data_c <- Toyota_data_class[Split_dataset==2,]
test_data_c <- Toyota_data_class[Split_dataset==3,]
head(training_data_c)
```

```{r class_tree}
classification_tree <- rpart(bin_price ~ Age_08_04 + KM + Fuel_Type.CNG + Fuel_Type.Diesel + 
                               Fuel_Type.Petrol  + HP + Automatic + Doors + Quarterly_Tax +
                               Mfr_Guarantee + Guarantee_Period + Airco + 
                               Automatic_airco + CD_Player + Powered_Windows + 
                               Sport_Model + Tow_Bar , data = training_data_c, method = "class")

summary(classification_tree)
plotcp(classification_tree)
prp(classification_tree, type = 1, extra = 1, split.font = 1, varlen = -10, cex = 0.5)
```

## i) Compare the tree generated by the CT with the one generated by the RT. Are they different? (Look at structure, the top predictors, size of tree, etc.) Why?

The 4 most important predictors are: Age_08_04, KM, HP, Automatic, and doors, The trees have the predictors, age, doors, HP  and KM in common. 
The differences between the two  trees are because the regression is based on average and classification is based on majority vote.


## ii. Predict the price, using the RT and the CT, of a used Toyota Corolla with the specifications listed in Table given in question.

```{r predictions1}
new_toyota_data <- data.frame(Age_08_04=77,Fuel_Type.CNG=0,
                              Fuel_Type.Diesel=0,Fuel_Type.Petrol=1,
                              KM=117000,HP=110,Automatic=0,Doors=5,
                              Quarterly_Tax=100,Mfr_Guarantee=0,
                              Guarantee_Period=3,Airco=1,Automatic_airco=0,
                              CD_Player=0,Powered_Windows=0,Sport_Model=0,Tow_Bar=1)

#Predictions
pred.regression <- predict(regression_tree,new_toyota_data)
pred.classification <- predict(classification_tree,new_toyota_data,type="class") 
```

```{r reg_score}
pred.regression
```


```{r class_pred}
pred.classification
```

```{r class_range}
levels(cut(Toyota_data_dummy$Price,20))[3]
```

The regression tree predicts the price to be 7955.584 and the classification tree bins the price in between the range of 7160 and 8570. 



# Question 2

```{r banks_data}
Banks<-read.xlsx("Banks.xlsx")

Banks$Financial.Condition<-factor(Banks$Financial.Condition,levels=c(0,1),
                                  labels=c("Strong","Weak"))
colnames(Banks)[5]<-'TotLnsLses/Assets'
```


```{r lr_model}
lr_model <- glm(Financial.Condition ~ `TotLnsLses/Assets` +
                  `TotExp/Assets`, data = Banks, family = "binomial")

summary(lr_model)


coef(lr_model)
exp(coef(lr_model))

```

## a. Write the estimated equation that associates the financial condition of a bank with its two predictors in three formats:

### i. The logit as a function of the predictors
  
$Logit(Financial Condition = weak) = -14.188 + 9.173\text{(TotLnsLses/Assets)} + 79.964\text{(TotExp/Assets)}$

### ii. The odds as a function of the predictors

$Odds(Financial Condition = weak) = e ^ {-14.188 + 9.173\text{(TotLnsLses/Assets)} + 79.964\text{(TotExp/Assets)}}$

### iii. The probability as a function of the predictors

$Probability (Financial Condition = weak) = \frac {e ^ {-14.188 + 9.173\text{(TotLnsLses/Assets)} + 79.964\text{(TotExp/Assets)}}} {1 + e ^ {-14.188 + 9.173\text{(TotLnsLses/Assets)} + 79.964\text{(TotExp/Assets)}}}$

## b. Consider a new bank whose total loans and leases/assets ratio = 0.6 and total
expenses/assets ratio = 0.11. From your logistic regression model, estimate the following four quantities for this bank: the logit, the odds, the probability of being financially weak, and the classification of the bank.

```{r preprocess2}
new_bank <- data.frame('TotExp/Assets'= 0.11, 'TotLnsLses/Assets' = 0.6)
colnames(new_bank)[1] <- "TotExp/Assets" 
colnames(new_bank)[2] <- "TotLnsLses/Assets"
new_bank_pred <- predict(lr_model, new_bank)
new_bank_pred

```

$Logit(Financial Condition = weak) = -14.188 + (9.173 * 0.6) + (79.964 * 0.11) = 0.11184$

$Odds(Financial Condition = weak) = e ^ {-14.188 + (9.173 * 0.6) + (79.964 * 0.11)} = 1.11183$

$Probability (Financial Condition = weak) = \frac {e ^ {-14.188 + (9.173 * 0.6) + (79.964 * 0.11)}} {1 + {e ^ {-14.188 + (9.173 * 0.6) + (79.964 * 0.11)}}} = 0.52793$


The Financial Condition is higher than 0.5, the cutoff value used, thus the new bank is considered to be successful.


## c. The cutoff value of 0.5 is used in conjunction with the probability of being financially weak. Compute the threshold that should be used if we want to make a classification based on the odds of being financially weak, and the threshold for the corresponding logit.

$Probability (Financial Condition = weak) = \frac {e ^ {-14.188 + 9.173\text{(TotLns\&Lses/Assets)} + 79.964\text{(TotExp/Assets)}}} {1 + e ^ {-14.188 + 9.173\text{(TotLns\&Lses/Assets)} + 79.964\text{(TotExp/Assets)}}} = 0.5$

Therefore : 

$e ^ {-14.188 + 9.173\text{(TotLnsLses/Assets)} + 79.964\text{(TotExp/Assets)}} = 1$

$Logit(Financial Condition = weak) = -14.188 + 9.173\text{(TotLnsLses/Assets)} + 79.964\text{(TotExp/Assets)} = 0 $

$Odds(Financial Condition = weak) = e ^ {-14.188 + 9.173\text{(TotLnsLses/Assets)} + 79.964\text{(TotExp/Assets)}} = 1$


The threshold based on the odds of being financially weak is [1, $\infty$), and the threshold based on the logit of being financially weak is [0, $\infty$).

## d. Interpret the estimated coefficient for the total loans & leases to total assets ratio (TotLns&Lses/Assets) in terms of the odds of being financially weak.
The estimated coefficient for the TotLns&Lses/Assets is 9.173. The higher the estimated coefficient is, higher probability of being financially weak.

The cutoff value should be less than 0.5. The new cutoff will get less poor financial condition banks misclassified as financially strong.

## e. When a bank that is in poor financial condition is misclassified as financially strong, the misclassification cost is much higher than when a financially strong bank is misclassified as weak. To minimize the expected cost of misclassification, should the cutoff value for classification (which is currently at 0.5) be increased or decreased?

The cutoff value should be less than 0.5. The new cutoff will get less poor financial condition banks misclassified as financially strong.

## Question 3

```{r read_sys_admin}
Sys_admin_data<-read.xlsx("System Administrators.xlsx")
```

## a. Create a scatterplot of Experience versus Training using color or symbol to differentiate programmers who complete the task from those who did not complete it. Which predictor(s) appear(s) potentially useful for classifying task completion?

```{r qqplotting}
qplot(x=Experience,y=Training,data=Sys_admin_data, 
      main = "Scatterplot for Experience v/s Training", colour = Completed.task)
```

From the scatterplot we can see that ‘Experience’ is a useful predictor for classifying task completion.


## b. Run a logistic regression model with both predictors using the entire dataset as training data. Among those who complete the task, what is the percentage of programmers who are incorrectly classified as failing to complete the task?

```{r gen_linear_model}

l_r_model <- glm(as.factor(Completed.task) ~ Experience + Training,
                 data = Sys_admin_data, family = "binomial") 
summary(l_r_model)

predict_task <- predict(l_r_model,Sys_admin_data,type = "response")
predict_task = ifelse(predict_task>0.5,'Yes','No')

confusionMatrix(data=as.factor(predict_task),reference=as.factor(Sys_admin_data[,3]),positive = 'Yes')


```
The percentage of programmers who are incorrectly classified as failing to complete the task is 5/15 * 100 = 33.33%


## c. To decrease the percentage in part (b), should the cutoff probability be increased or decreased?

To decrease the percentage error of programmers who are incorrectly classified as failing to complete the task, the cutoff value should be decreased.


## d. How much experience must be accumulated by a programmer with 4 years of training before his or her estimated probability of completing the task exceeds 50%?

As the probability is more than 0.5 at 9.25 years therefore programmer requires 9.25+ years of experience.


